{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf7cbe6540dd4dc5b99296b22e9e88cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96130dc70cf4484fb84ba293d6d93ce2",
              "IPY_MODEL_f6d49f02dad244c0a2cea31ddeac45ae"
            ],
            "layout": "IPY_MODEL_b34f35a284a14e58a495e38fa073e027"
          }
        },
        "96130dc70cf4484fb84ba293d6d93ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5abfc31db6e44588d242d5767b32d14",
            "placeholder": "​",
            "style": "IPY_MODEL_99c12a47892a48b7a69eb28aa0dc3a97",
            "value": "0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "f6d49f02dad244c0a2cea31ddeac45ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a4cc5188d8b4230b2e70b768222f870",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fed6f82bee44b75871d86bfcaf1405a",
            "value": 1
          }
        },
        "b34f35a284a14e58a495e38fa073e027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5abfc31db6e44588d242d5767b32d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c12a47892a48b7a69eb28aa0dc3a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a4cc5188d8b4230b2e70b768222f870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fed6f82bee44b75871d86bfcaf1405a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Statements"
      ],
      "metadata": {
        "id": "MeqLhPeZ-KgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHedW0Sg9OWJ"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "Z9Vu66XSK1wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "EgV5h4O3mtYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "Kj9Q6U7--PWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zlvNB4o-H5B",
        "outputId": "0f8796d0-2e35-4284-f1ea-7ae07fc680b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip data to local session\n",
        "%%capture\n",
        "!unzip \"/content/drive/MyDrive/aksharantar_sampled.zip\""
      ],
      "metadata": {
        "id": "lSzg7ztV-x3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect all languages\n",
        "datafolder_path = \"/content/aksharantar_sampled/\"\n",
        "languages = [name for name in os.listdir(datafolder_path) if os.path.isdir(os.path.join(datafolder_path, name))]\n",
        "# print(languages)"
      ],
      "metadata": {
        "id": "4BVGfzjI_J5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data\n",
        "allData = {}\n",
        "language = 'tam'\n",
        "languages = ['tam']\n",
        "datasetCategories = [\"train\", \"test\", \"valid\"]\n",
        "dataPath = datafolder_path + language + \"/\"\n",
        "allData[language] = {}\n",
        "for category in datasetCategories:\n",
        "    allData[language][category] = pd.read_csv(dataPath + language + \"_\" + category + \".csv\", header = None)"
      ],
      "metadata": {
        "id": "gssaGpHcB7vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = []\n",
        "for language in languages:\n",
        "    for idx in range(len(allData[language]['train'][0])):\n",
        "        pair = [allData[language]['train'][0][idx], allData[language]['train'][1][idx]]\n",
        "        train_pairs.append(pair)\n",
        "\n",
        "val_pairs = []\n",
        "for language in languages:\n",
        "    for idx in range(len(allData[language]['valid'][0])):\n",
        "        pair = [allData[language]['valid'][0][idx], allData[language]['valid'][1][idx]]\n",
        "        val_pairs.append(pair)\n",
        "\n",
        "test_pairs = []\n",
        "for language in languages:\n",
        "    for idx in range(len(allData[language]['test'][0])):\n",
        "        pair = [allData[language]['test'][0][idx], allData[language]['test'][1][idx]]\n",
        "        test_pairs.append(pair)"
      ],
      "metadata": {
        "id": "e6u-9Vu5G4oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Language Model"
      ],
      "metadata": {
        "id": "vFngwpCBE7oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LangModel:\n",
        "    def __init__(self, name = \"devanagiri\"):\n",
        "        self.name = name\n",
        "        self.character2index = {\"SOW\" : 0, \"EOW\" : 1}\n",
        "        self.index2character = {0 : \"SOW\", 1 : \"EOW\"}\n",
        "        self.nCharacters = 2\n",
        "        self.character2count = {}\n",
        "\n",
        "    def addWord(self, word):\n",
        "        for character in word:\n",
        "            self.addCharacter(character)\n",
        "    \n",
        "    def addCharacter(self, character):\n",
        "        if character not in self.character2index:\n",
        "            self.character2index[character] = self.nCharacters\n",
        "            self.character2count[character] = 1\n",
        "            self.index2character[self.nCharacters] = character\n",
        "            self.nCharacters += 1\n",
        "        else:\n",
        "            self.character2count[character] += 1\n"
      ],
      "metadata": {
        "id": "Yz2jFxRfE-Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(allData, languages):\n",
        "    inputModel = LangModel(name = 'eng')\n",
        "    outputModel = LangModel()\n",
        "    for language in languages:\n",
        "        nSamples = len(allData[language]['train'])\n",
        "        for i in range(nSamples):\n",
        "            inputModel.addWord(allData[language]['train'][0][i])\n",
        "            outputModel.addWord(allData[language]['train'][1][i])\n",
        "    return inputModel, outputModel"
      ],
      "metadata": {
        "id": "XWSbnPhbOcVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang = prepareData(allData, languages)"
      ],
      "metadata": {
        "id": "ejaDYRVxQDrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Training Data"
      ],
      "metadata": {
        "id": "Mt12UyzlV6V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromWord(lang, word):\n",
        "    return [lang.character2index[character] for character in word]\n",
        "\n",
        "def tensorFromWord(lang, word):\n",
        "    indexes = indexesFromWord(lang, word)\n",
        "    indexes.append(lang.character2index[\"EOW\"])\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
        "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "0jq0XRlrV5lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2Seq Model with Attention"
      ],
      "metadata": {
        "id": "5gVN8c4zEUON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = input_lang.nCharacters + 10\n",
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "metadata": {
        "id": "ookkzSJQDHSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers = 3, embedding_size = 32, bidirectional = False, cell_type = \"GRU\"):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.cell_type = cell_type\n",
        "        if cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, bidirectional = bidirectional, num_layers = num_layers)\n",
        "        else:\n",
        "            self.gru = nn.GRU(embedding_size, hidden_size, bidirectional = bidirectional, num_layers = num_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # print(\"Encoder\")\n",
        "        # print(\"Input shape :\", input.shape)\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        # print(\"Embedded shape :\", embedded.shape)\n",
        "        output = embedded\n",
        "        if self.cell_type == \"RNN\":\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        else:\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "        # print(\"output shape :\", output.shape)\n",
        "        # print(\"hidden shape :\", hidden.shape)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "OR-lXZVhET7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0, bidirectional = False, num_layers = 3, embedding_size = 32, cell_type = \"GRU\", max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_size = embedding_size\n",
        "        self.cell_type = \"GRU\"\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
        "        self.attn = nn.Linear(self.hidden_size + self.embedding_size, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        if cell_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(hidden_size, hidden_size, bidirectional = bidirectional, num_layers = num_layers)\n",
        "        else:\n",
        "            self.gru = nn.GRU(hidden_size, hidden_size, bidirectional = bidirectional, num_layers = num_layers)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        if self.cell_type == \"RNN\":\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        else:\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "ltqx_zJECyK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "nJc7oDuZeiin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = input_lang.nCharacters + 10\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "2MAPnjBbd6R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(pairs, encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
        "    plot_losses = []\n",
        "    plot_valacc = []\n",
        "    plot_testacc = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            # print(\"Iteration :\", iter)\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "\n",
        "            # val_acc = calc_acc(encoder, decoder, val_pairs)\n",
        "            plot_losses.append(print_loss_avg)\n",
        "            # plot_valacc.append(val_acc)\n",
        "            # wandb.log({\"train_loss\" : print_loss_avg})\n",
        "            # print(\"Loss :\", print_loss_avg)\n",
        "            \n",
        "            val_acc = calc_acc(encoder, decoder, val_pairs)\n",
        "            plot_valacc.append(val_acc)\n",
        "            # wandb.log({\"val_acc\" : val_acc})\n",
        "            # print(\"Val Acc:\", val_acc)\n",
        "\n",
        "            # test_acc = calc_acc(encoder, decoder, test_pairs)\n",
        "            # plot_testacc.append(test_acc)\n",
        "            print(iter, print_loss_avg, val_acc)\n",
        "\n",
        "    return plot_losses, plot_valacc, plot_testacc\n",
        "  "
      ],
      "metadata": {
        "id": "-h7JyOrue7mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, word, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromWord(input_lang, word)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2character[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "i_qGW3Y2Hk83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_acc(encoder, decoder, data_pairs):\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    for pair in data_pairs:\n",
        "        pred = \"\"\n",
        "        for c in evaluate(encoder, decoder, pair[0])[0][:-1]:\n",
        "            pred += c\n",
        "        num_total += 1\n",
        "        if(pred == pair[1]):\n",
        "            num_correct += 1\n",
        "    return num_correct/num_total"
      ],
      "metadata": {
        "id": "EbIwOdYKF6ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_size = 256\n",
        "# encoder1 = EncoderRNN(input_lang.nCharacters, hidden_size, bidirectional = False).to(device)\n",
        "# decoder1 = AttnDecoderRNN(hidden_size, output_lang.nCharacters, bidirectional = False).to(device)\n",
        "\n",
        "# trainIters(train_pairs, encoder1, decoder1, 25000, print_every=2500, learning_rate = 0.01)"
      ],
      "metadata": {
        "id": "4KOVJNexo0KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(pairs, encoder, decoder, n=10):\n",
        "    predictions = []\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        # print('>', pair[0])\n",
        "        # print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_Word = ''.join(output_words[:-1])\n",
        "        # print('<', output_Word)\n",
        "        # print('')\n",
        "        predictions.append([pair[0], pair[1], output_Word, (pair[1] == output_Word)])\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "3WSNsznTfIT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateDataset(pairs, encoder, decoder):\n",
        "    predictions = []\n",
        "    for pair in pairs:\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_Word = ''.join(output_words[:-1])\n",
        "\n",
        "        predictions.append([pair[0], pair[1], output_Word, (pair[1] == output_Word)])\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "66viWlZ8Sui6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wandb Sweeps"
      ],
      "metadata": {
        "id": "wnawYBmdKwQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9YCKmp2hLJXx",
        "outputId": "4216b94e-339c-44fd-8de2-db2e81d6456c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"Best Model Run - with attention\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \n",
        "        \"num_layers\" : {\"values\" : [1, 2, 3]},\n",
        "\n",
        "        \"cell_type\" : {\"values\" : [\"GRU\"]},\n",
        "\n",
        "        \"dropout\" : {\"values\" : [0, 0.2, 0.3]},\n",
        "\n",
        "        \"embedding_size\" : {\"values\" : [32, 128, 256]},\n",
        "\n",
        "        \"learning_rate\" : {\"values\" : [0.001, 0.01]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "DDOR8PHZKwCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_model(config = None):\n",
        "    \n",
        "    with wandb.init(config = config, project = \"CS6910-A3\") as run:\n",
        "        \n",
        "        config = wandb.config\n",
        "\n",
        "        name_str = \"nl_\" + str(config['num_layers']) + \"_\" + str(config['cell_type']) + \"_d_\" + str(config['dropout']) + \"_es_\" + str(config['embedding_size']) + \"_lr_\" + str(config['learning_rate'])\n",
        "        run.name = name_str\n",
        "\n",
        "        embedding_size = config['embedding_size']\n",
        "        bidirectional = False\n",
        "        dropout_p = config['dropout']\n",
        "        cell_type = config['cell_type']\n",
        "        num_layers = config['num_layers']\n",
        "        learning_rate = config['learning_rate']\n",
        "        hidden_size = 256\n",
        "\n",
        "        encoder = EncoderRNN(input_lang.nCharacters, hidden_size, num_layers, embedding_size, bidirectional, cell_type).to(device)\n",
        "        decoder = AttnDecoderRNN(hidden_size, output_lang.nCharacters, dropout_p, bidirectional, num_layers, embedding_size, cell_type).to(device)\n",
        "\n",
        "        # We will train for 25000 iterations and select the most promising\n",
        "        losses, valaccs, testaccs = trainIters(train_pairs, encoder, decoder, 50000, print_every = 5000, learning_rate = learning_rate)\n",
        "\n",
        "        for idx in range(len(losses)):\n",
        "            wandb.log({\"train_loss\" : losses[idx],\n",
        "                       \"val_acc\" : valaccs[idx],\n",
        "                       \"iterations\" : 100*(idx + 1)})\n",
        "            \n",
        "        # Make Predictions\n",
        "        # predictions = evaluateRandomly(test_pairs, encoder, decoder, 1000)\n",
        "        # my_df = pd.DataFrame(predictions)\n",
        "        # my_df.to_csv(\"predictions.csv\", index = False, header = False)\n"
      ],
      "metadata": {
        "id": "xzgHWDuxM1fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project = \"CS6910-A3\")"
      ],
      "metadata": {
        "id": "iaeeeeFWs1M-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8b1b0a-b397-472c-9ad3-83c53d2977f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: bojd2x15\n",
            "Sweep URL: https://wandb.ai/mani-ml/CS6910-A3/sweeps/bojd2x15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = wandb.agent(sweep_id, function = create_and_train_model, project = \"CS6910-A3\", count = 30)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "h3_CLhkqspAc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "bf7cbe6540dd4dc5b99296b22e9e88cb",
            "96130dc70cf4484fb84ba293d6d93ce2",
            "f6d49f02dad244c0a2cea31ddeac45ae",
            "b34f35a284a14e58a495e38fa073e027",
            "a5abfc31db6e44588d242d5767b32d14",
            "99c12a47892a48b7a69eb28aa0dc3a97",
            "7a4cc5188d8b4230b2e70b768222f870",
            "0fed6f82bee44b75871d86bfcaf1405a"
          ]
        },
        "outputId": "cf148e1d-2b1b-4fc9-fe2c-19bf30a89e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cbn7ougz with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230521_050219-cbn7ougz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mani-ml/CS6910-A3/runs/cbn7ougz' target=\"_blank\">chocolate-sweep-2</a></strong> to <a href='https://wandb.ai/mani-ml/CS6910-A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/mani-ml/CS6910-A3/sweeps/bojd2x15' target=\"_blank\">https://wandb.ai/mani-ml/CS6910-A3/sweeps/bojd2x15</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mani-ml/CS6910-A3' target=\"_blank\">https://wandb.ai/mani-ml/CS6910-A3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/mani-ml/CS6910-A3/sweeps/bojd2x15' target=\"_blank\">https://wandb.ai/mani-ml/CS6910-A3/sweeps/bojd2x15</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mani-ml/CS6910-A3/runs/cbn7ougz' target=\"_blank\">https://wandb.ai/mani-ml/CS6910-A3/runs/cbn7ougz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000 2.649092268931747 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf7cbe6540dd4dc5b99296b22e9e88cb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AfrosxG32vhA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}